# N√°pady na roz≈°√≠≈ôen√≠ SmartScan - Embedding Features

## üìã P≈ôehled

Tento dokument obsahuje kompletn√≠ anal√Ωzu **co v≈°echno lze vektorovat** pro s√©mantick√© vyhled√°v√°n√≠ v SmartScan aplikaci. N√°pady jsou prioritizovan√© podle hodnoty, implementaƒçn√≠ n√°roƒçnosti a F-Droid kompatibility.

**Datum anal√Ωzy:** 2025-10-27
**Aktu√°ln√≠ verze:** v1.1.3
**Pl√°novan√° verze:** v1.2.0 (Document Search)

---

## üéØ TIER 1: TOP PRIORITY - Ihned implementovateln√©

### 1. OCR Embeddings (Screenshots, Business Cards, Handwritten Notes) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** üü¢ HIGHLY RECOMMENDED - Nejlep≈°√≠ value/effort ratio

**Model: PaddleOCR**
- **Velikost:** 2.8-3.5 MB (ultra-lightweight!)
- **ONNX:** ‚úÖ YES - [RapidAI Conversion](https://github.com/RapidAI/PaddleOCRModelConvert)
- **F-Droid:** ‚úÖ Apache 2.0
- **Performance:** <100ms per image
- **Licence:** Apache 2.0 (F-Droid compatible)

**Use cases:**
1. **Screenshot search:**
   - "Najdi screenshot s settings"
   - "Find screenshot with password reset email"
   - "Screenshot kde je QR code"

2. **Business card organization:**
   - Extrakce jm√©no, telefon, email, firma
   - Automatic contact creation
   - Search by company name

3. **Handwritten notes:**
   - OCR + embeddings
   - Search by content
   - Note organization

4. **Document text extraction:**
   - Combine with existing PDF search
   - Scanned documents support
   - Receipt/invoice organization

**Proƒç je to KILLER FEATURE:**
- ‚úÖ Ka≈æd√Ω m√° stovky screenshot≈Ø - nikdo je neum√≠ naj√≠t
- ‚úÖ OCR text ‚Üí embeddings ‚Üí semantic search
- ‚úÖ N√≠zk√° konkurence (m√°lo Android apps to m√°)
- ‚úÖ Mal√Ω model (3 MB)
- ‚úÖ F-Droid compatible

**Technick√° implementace:**
```kotlin
// Pipeline
Screenshot Image
    ‚Üì
PaddleOCR (text extraction) - 2.8 MB
    ‚Üì
Text Chunks
    ‚Üì
MiniLM-L6-v2 (embeddings) - 23 MB (u≈æ bude≈° m√≠t)
    ‚Üì
Searchable Index
```

**Implementaƒçn√≠ n√°roƒçnost:** ST≈òEDN√ç
- Integrate PaddleOCR ONNX model
- Reuse existing text embedding pipeline (z document search)
- UI: Screenshot viewer + OCR overlay

**Dependencies:**
- ‚úÖ Text embeddings (v1.2.0 - Document Search)
- ‚úÖ ONNX Runtime (u≈æ m√°≈°)
- ‚úÖ Image loading (Coil, u≈æ m√°≈°)

**Estimated effort:** 2-3 t√Ωdny

**GitHub References:**
- [PaddleOCR Official](https://github.com/PaddlePaddle/PaddleOCR)
- [PaddleOCR ONNX Conversion](https://github.com/RapidAI/PaddleOCRModelConvert)
- [Paddle2ONNX Docs](https://paddlepaddle.github.io/PaddleOCR/main/en/version3.x/deployment/obtaining_onnx_models.html)

---

### 2. Document Search (PDF, Markdown, TXT) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** üü° IN PROGRESS - Pl√°nov√°no pro v1.2.0

**Model: all-MiniLM-L6-v2**
- **Velikost:** 23 MB
- **ONNX:** ‚úÖ YES
- **F-Droid:** ‚úÖ Apache 2.0
- **Performance:** ~60ms per 512-token chunk

**Detaily:** Viz `dokumenty.md`

**Estimated effort:** 5-6 t√Ωdn≈Ø (part-time)

---

### 3. Audio Embeddings (Voice Memos, Podcasts, Audiobooks) ‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** üü° FEASIBLE - Mo≈æn√© po v1.2.0

**Model: Wav2Vec 2.0**
- **Velikost:** 50-100 MB (z√°vis√≠ na variantƒõ)
- **ONNX:** ‚úÖ YES - [HuggingFace Collection](https://huggingface.co/darjusul/wav2vec2-ONNX-collection)
- **F-Droid:** ‚úÖ Apache 2.0 / MIT
- **Performance:** Real-time capable (s constraints)

**Use cases:**
1. **Voice memo search:**
   - "Najdi nahr√°vku o projektu X"
   - Semantic search v audio obsahu
   - Transcription + embeddings

2. **Podcast organization:**
   - Search by topic/content
   - Episode recommendations
   - Semantic grouping

3. **Speaker recognition:**
   - Group recordings by speaker
   - "Find all recordings of person X"
   - Speaker similarity

4. **Audio similarity:**
   - Find similar music/sounds
   - Duplicate audio detection
   - Genre classification

**Technick√° implementace:**
```kotlin
// Pipeline
Audio File
    ‚Üì
Wav2Vec 2.0 (audio embeddings) - 50-100 MB
    ‚Üì
384-dim embeddings
    ‚Üì
Cosine similarity search

// Optional: Combine with transcription
Audio File
    ‚Üì
Whisper (transcription)
    ‚Üì
MiniLM-L6-v2 (text embeddings)
    ‚Üì
Multi-modal search
```

**V√Ωhody:**
- ‚úÖ Voice memo search (jedineƒçn√° feature)
- ‚úÖ Podcast/audiobook organization
- ‚úÖ F-Droid compatible
- ‚úÖ ONNX Runtime compatible

**Nev√Ωhody:**
- ‚ö†Ô∏è Model size: 50-100 MB (vƒõt≈°√≠ ne≈æ text embeddings)
- ‚ö†Ô∏è Processing time: Audio je del≈°√≠ ne≈æ text/images
- ‚ö†Ô∏è Memory consumption: Audio buffering

**Implementaƒçn√≠ n√°roƒçnost:** VYSOK√Å
- Audio decoding (Media3 ExoPlayer, u≈æ m√°≈°)
- Wav2Vec2 ONNX integration
- Batch processing pro long audio files
- Background worker (WorkManager)
- UI: Audio player + search results

**Estimated effort:** 3-4 t√Ωdny

**GitHub References:**
- [Wav2Vec2 ONNX Collection](https://huggingface.co/darjusul/wav2vec2-ONNX-collection)
- [Wespeaker (Speaker Embeddings)](https://github.com/wenet-e2e/wespeaker)
- [Audio Processing Research 2024](https://www.sciencedirect.com/science/article/abs/pii/S0167639324000761)

---

### 4. Metadata Embeddings (EXIF, File Properties) ‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** üü¢ EASY - Reuse existing infrastructure

**Model: all-MiniLM-L6-v2** (reuse z document search)
- **Velikost:** 0 MB (u≈æ bude≈° m√≠t model)
- **ONNX:** ‚úÖ YES
- **F-Droid:** ‚úÖ Apache 2.0

**Use cases:**
1. **Camera settings search:**
   - "Photos shot at f/1.8"
   - "Images with ISO 3200+"
   - "Photos from 50mm lens"

2. **Location descriptions:**
   - "Photos from Prague"
   - "Beach vacation photos"
   - "Mountain hiking"

3. **Time-based queries:**
   - "Summer 2024 photos"
   - "Morning photos"
   - "Weekend pictures"

4. **Device/camera search:**
   - "Photos from Canon EOS"
   - "Smartphone photos"
   - "Drone footage"

**Technick√° implementace:**
```kotlin
// Extract EXIF metadata
val metadata = """
Camera: Canon EOS R5
Lens: RF 50mm f/1.2
Settings: f/1.8, 1/500s, ISO 400
Location: Prague, Czech Republic
Date: 2024-08-15 14:30
"""

// Convert to embeddings (reuse MiniLM-L6-v2)
val embedding = textEmbedModel.encode(metadata)

// Search: "photos from Canon with shallow depth of field"
val query = "Canon photos with shallow depth of field"
val queryEmbedding = textEmbedModel.encode(query)
val results = cosineSimilarity(queryEmbedding, allMetadataEmbeddings)
```

**V√Ωhody:**
- ‚úÖ Zero additional model size
- ‚úÖ Semantic search v metadata
- ‚úÖ Natural language queries
- ‚úÖ Reuse existing infrastructure

**Implementaƒçn√≠ n√°roƒçnost:** N√çZK√Å
- EXIF extraction (Android MediaMetadataRetriever, u≈æ m√°≈°)
- Format metadata jako text
- Reuse text embedding model
- Minimal UI changes

**Estimated effort:** 1 t√Ωden

---

### 5. Video Content Embeddings (Frame-level + Transcript) ‚≠ê‚≠ê‚≠ê

**Status:** üü° PARTIAL - M≈Ø≈æe≈° reuse existing CLIP

**Approach:** Kombinace st√°vaj√≠c√≠ch n√°stroj≈Ø
- **Frame embeddings:** CLIP (u≈æ m√°≈°!)
- **Audio transcript:** Whisper + text embeddings (optional)
- **Combined search:** Multi-modal similarity

**Use cases:**
1. **Frame-level search:**
   - "Find video with blue car"
   - "Video showing cooking"
   - "Sunset timelapses"

2. **Transcript search** (optional):
   - "Video where person says 'machine learning'"
   - Search v dialogu/narration

3. **Scene detection:**
   - Group similar scenes
   - Find duplicate videos
   - Highlight detection

**Technick√° implementace:**
```kotlin
// Already implemented (v1.1.3)
Video File
    ‚Üì
Frame extraction (every X seconds) - Media3 ExoPlayer
    ‚Üì
CLIP embeddings per frame
    ‚Üì
Average/max pooling
    ‚Üì
Video-level embedding

// NEW: Add transcript embeddings
Video File
    ‚Üì
Audio extraction
    ‚Üì
Whisper (transcription) - optional
    ‚Üì
MiniLM-L6-v2 (text embeddings)
    ‚Üì
Combined search (frame + transcript)
```

**V√Ωhody:**
- ‚úÖ Frame embeddings u≈æ m√°≈° (CLIP)
- ‚úÖ Infrastructure ready (video processing)
- ‚úÖ Incremental improvement

**Nev√Ωhody:**
- ‚ö†Ô∏è Whisper model size (varies: 40 MB - 1.5 GB)
- ‚ö†Ô∏è Transcription latency
- ‚ö†Ô∏è Language support (English focus)

**Implementaƒçn√≠ n√°roƒçnost:** ST≈òEDN√ç
- U≈æ m√°≈° frame extraction + CLIP
- P≈ôidej: Whisper integration (optional)
- Combine embeddings (weighted average)

**Estimated effort:** 2 t√Ωdny (jen transcript addition)

**GitHub References:**
- [Whisper ONNX Models](https://github.com/openai/whisper)
- [Whisper.cpp (lightweight)](https://github.com/ggerganov/whisper.cpp)

---

### 6. UI Element Embeddings (Screenshot Analysis) ‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** üü¢ EASY - Reuse existing CLIP

**Model: CLIP** (u≈æ m√°≈°!)
- **Velikost:** 0 MB (already have)
- **ONNX:** ‚úÖ YES
- **F-Droid:** ‚úÖ Already in project

**Use cases:**
1. **UI element search:**
   - "Find screenshot with blue button"
   - "Screenshots showing settings screen"
   - "App with navigation drawer"

2. **App identification:**
   - "Find Instagram screenshots"
   - "Screenshots from Chrome browser"
   - Visual app detection

3. **UI pattern recognition:**
   - "Find login screens"
   - "Screenshots with forms"
   - "Error messages"

**Technick√° implementace:**
```kotlin
// Use existing CLIP embeddings
Screenshot Image
    ‚Üì
CLIP image encoder (u≈æ m√°≈°!)
    ‚Üì
Image embedding
    ‚Üì
Query: "blue settings button"
    ‚Üì
CLIP text encoder
    ‚Üì
Cosine similarity
```

**V√Ωhody:**
- ‚úÖ Zero additional implementation
- ‚úÖ Already have CLIP model
- ‚úÖ Works out-of-the-box

**Kombinace s OCR:**
```kotlin
// Best approach: Combine CLIP + OCR
Screenshot
    ‚Üì
    ‚îú‚îÄ‚Üí CLIP (visual features) - "blue button", "settings UI"
    ‚îî‚îÄ‚Üí OCR (text content) - "WiFi password: 12345"
    ‚Üì
Combined embedding
    ‚Üì
Multi-modal search
```

**Implementaƒçn√≠ n√°roƒçnost:** VELMI N√çZK√Å
- Pouze UI/UX improvements
- Query suggestions ("Find screenshot with...")
- No new models needed

**Estimated effort:** 3-5 dn≈Ø

---

## üî¨ TIER 2: POKROƒåIL√â - Mo≈æn√©, ale slo≈æitƒõj≈°√≠

### 7. MobileCLIP (Upgrade souƒçasn√©ho CLIP modelu) ‚≠ê‚≠ê‚≠ê‚≠ê

**Status:** üü° RESEARCH NEEDED - License unclear

**Apple ML Research (2024)**
- **Velikost:**
  - MobileCLIP-S0: ~40 MB (nejmen≈°√≠, 4.8x rychlej≈°√≠)
  - MobileCLIP-S2: ~80 MB (balanced, 2.3x rychlej≈°√≠)
  - MobileCLIP-B(LT): 173 MB (nejvy≈°≈°√≠ kvalita)
- **ONNX:** ‚ö†Ô∏è Mo≈æn√© (CoreML ‚Üí ONNX conversion)
- **F-Droid:** ‚ö†Ô∏è **LICENSE NEEDS RESEARCH**
- **Performance:**
  - Latency: 3.6ms (image) + 3.3ms (text) na iPhone 12
  - 2-5x rychlej≈°√≠ ne≈æ standardn√≠ CLIP
  - 2.8x men≈°√≠ ne≈æ ViT-B/16

**V√Ωhody:**
- ‚úÖ 2-5x rychlej≈°√≠ inference
- ‚úÖ Men≈°√≠ model size
- ‚úÖ Lep≈°√≠ accuracy (v nƒõkter√Ωch benchmarks)
- ‚úÖ Lower power consumption

**Nev√Ωhody:**
- ‚ö†Ô∏è Apple license unclear (research needed)
- ‚ö†Ô∏è CoreML ‚Üí ONNX conversion complexity
- ‚ö†Ô∏è Less mature ne≈æ OpenAI CLIP

**Upgrade path:**
```
Current: OpenAI CLIP (~150 MB, ~30ms latency)
    ‚Üì
Replace with: MobileCLIP-S0 (~40 MB, ~7ms latency)
    ‚Üì
Result: 3x men≈°√≠, 4x rychlej≈°√≠, lep≈°√≠ battery life
```

**Implementaƒçn√≠ n√°roƒçnost:** ST≈òEDN√ç-VYSOK√Å
- Research Apple ML license
- CoreML ‚Üí ONNX conversion
- Benchmark comparison (accuracy, speed)
- Migration from CLIP ‚Üí MobileCLIP
- Regression testing

**Estimated effort:** 2-3 t√Ωdny (vƒçetnƒõ research)

**Action Items:**
- [ ] Research MobileCLIP license (MIT? Apache? Proprietary?)
- [ ] Test CoreML ‚Üí ONNX conversion
- [ ] Benchmark vs current CLIP
- [ ] F-Droid compatibility check

**GitHub References:**
- [MobileCLIP GitHub](https://github.com/apple/ml-mobileclip)
- [Apple ML Research](https://machinelearning.apple.com/research/mobileclip)
- [arXiv Paper](https://arxiv.org/abs/2311.17049)

---

### 8. Music Embeddings ‚≠ê‚≠ê

**Status:** üî¥ NOT FEASIBLE - Too heavyweight

**Modely:**
- **Jukebox (OpenAI):** >1 GB model size
- **MusicGen (Meta):** >500 MB model size
- **MusicLM (Google):** Heavy model
- **ONNX status:** In progress (2024) - [GitHub Issue](https://github.com/huggingface/optimum/issues/1297)

**Use cases:**
1. Music library organization
2. "Find similar songs"
3. Genre classification
4. Mood-based search

**Proƒç NE:**
- ‚ùå Model size >1 GB (impractical pro mobile)
- ‚ùå ONNX conversion nen√≠ ready
- ‚ùå Inference latency vysok√°
- ‚ùå Battery consumption significant

**Alternativa:**
- Music metadata embeddings (artist, genre, lyrics)
- Pou≈æij text embeddings (MiniLM-L6-v2)
- Acoustic features (tempo, key) + lightweight ML

**Verdict:** Zat√≠m **NE** - wait for lightweight models

**GitHub References:**
- [Jukebox Embeddings Dataset](https://huggingface.co/datasets/jonflynn/musicnet_jukebox_embeddings)
- [MusicGen (Meta)](https://huggingface.co/facebook/musicgen-melody)
- [MusicLM PyTorch](https://github.com/lucidrains/musiclm-pytorch)

---

### 9. Time Series Embeddings (User Behavior Patterns) ‚≠ê‚≠ê

**Status:** üü° INTERESTING - Ale ne core feature

**Modely (2024-2025):**
- **Tiny Time Mixers (IBM)** - Released June 2024
  - MLP-based (ne transformer)
  - Lightweight pro edge deployment
  - Adaptive patching

- **Foundation Models:**
  - TimesFM (Google)
  - Chronos (Amazon)
  - Moirai (Salesforce)
  - TimeGPT

**ONNX:** ‚úÖ Conversions podporov√°ny (ONNX/TensorRT)

**Use cases:**
1. **Photo-taking behavior:**
   - "When do you usually take photos?"
   - Temporal pattern detection
   - Predict next photo session

2. **App usage patterns:**
   - App launch sequences
   - Usage prediction
   - Behavioral insights

3. **Location patterns:**
   - Regular routes
   - Favorite places
   - Travel pattern detection

4. **Sensor data embeddings:**
   - Accelerometer/gyroscope patterns
   - Activity recognition
   - Context awareness

**Technick√° implementace:**
```kotlin
// Time series data
val photoTimestamps = listOf(
    "2024-08-15 14:30",
    "2024-08-15 14:35",
    "2024-08-20 09:00",
    // ...
)

// Extract temporal features
val features = extractTemporalFeatures(photoTimestamps)

// Time series embedding
val embedding = timeSeriesModel.encode(features)

// Pattern detection
val patterns = detectPatterns(embedding)
// Result: "You usually take photos on weekends between 2-5 PM"
```

**V√Ωhody:**
- ‚úÖ Unique insights
- ‚úÖ Behavioral recommendations
- ‚úÖ Lightweight models exist

**Nev√Ωhody:**
- ‚ö†Ô∏è Privacy concerns (tracking behavior)
- ‚ö†Ô∏è Limited use case pro media app
- ‚ö†Ô∏è Complexity vs value

**Relevance:** ST≈òEDN√ç - zaj√≠mav√©, ale ne core feature

**Implementaƒçn√≠ n√°roƒçnost:** VYSOK√Å
- Time series feature engineering
- Model selection & integration
- Privacy-preserving design
- UI/UX pro insights

**Estimated effort:** 3-4 t√Ωdny

**Verdict:** Zaj√≠mav√© pro **future research**, ale ne pro nearest releases

**GitHub References:**
- [Tiny Time Mixers (IBM)](https://huggingface.co/ibm/TTM)
- [TimesFM (Google)](https://github.com/google-research/timesfm)
- [Chronos (Amazon)](https://github.com/amazon-science/chronos-forecasting)

---

## ‚ùå TIER 3: NE PRO SMARTSCAN

### Code Embeddings (CodeBERT, GraphCodeBERT)

**Proƒç NE:**
- ‚ùå Irelevantn√≠ pro media organization app
- ‚ùå Wrong domain

**Model:** CodeBERT (Microsoft)
- **GitHub:** [microsoft/CodeBERT](https://github.com/microsoft/CodeBERT)
- **Use case:** Code search, documentation

**Verdict:** Skip

---

### 3D Model Embeddings

**Proƒç NE:**
- ‚ùå Irelevantn√≠ pro SmartScan
- ‚ùå V√Ωzkumn√° oblast
- ‚ùå Heavy models

**Verdict:** Skip

---

### Graph Neural Networks (Knowledge Graphs)

**Proƒç NE:**
- ‚ùå P≈ô√≠li≈° komplexn√≠
- ‚ùå Overkill pro SmartScan
- ‚ùå Implementaƒçn√≠ overhead

**Use cases (theoretical):**
- Contact relationship graphs
- Photo social networks
- App usage graphs

**Model:** GNN embeddings
- **Research:** [Nature Scientific Reports 2025](https://www.nature.com/articles/s41598-025-05260-1)
- **ONNX:** Supported

**Verdict:** Zaj√≠mav√© pro research, ale ne praktick√©

---

### Medical/Legal/Scientific Domain Embeddings

**Proƒç NE:**
- ‚ùå Wrong domain
- ‚ùå Irelevantn√≠

**Modely:**
- MedCLIP (medical imaging)
- BioBERT (clinical notes)
- LegalBERT (legal documents)
- SciBERT (scientific papers)

**Verdict:** Skip

---

## üìä PRIORITY MATRIX

| Feature | Model | Size | F-Droid | Value | Effort | Priority |
|---------|-------|------|---------|-------|--------|----------|
| **OCR embeddings** | PaddleOCR | 3 MB | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | St≈ôedn√≠ | **#1** |
| **Document search** | MiniLM-L6-v2 | 23 MB | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Vysok√Ω | **#2** (v1.2.0) |
| **Metadata search** | MiniLM-L6-v2 | 0 MB | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | N√≠zk√Ω | **#3** |
| **UI element search** | CLIP (existing) | 0 MB | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | Velmi n√≠zk√Ω | **#4** |
| **Audio embeddings** | Wav2Vec2 | 50-100 MB | ‚úÖ | ‚≠ê‚≠ê‚≠ê‚≠ê | Vysok√Ω | **#5** |
| **Video transcripts** | Whisper | varies | ‚úÖ | ‚≠ê‚≠ê‚≠ê | Vysok√Ω | **#6** |
| **MobileCLIP upgrade** | MobileCLIP-S0 | 40 MB | ‚ö†Ô∏è | ‚≠ê‚≠ê‚≠ê‚≠ê | St≈ôedn√≠ | **#7** (research) |
| **Time series** | TTM/TimesFM | varies | ‚úÖ | ‚≠ê‚≠ê | Vysok√Ω | #8 (future) |
| **Music embeddings** | Jukebox | >1 GB | ‚úÖ | ‚≠ê‚≠ê | Velmi vysok√Ω | ‚ùå (too heavy) |

---

## üó∫Ô∏è IMPLEMENTAƒåN√ç ROADMAP

### **v1.2.0 - Document Search** (aktu√°lnƒõ v pl√°nu)
- ‚úÖ PDF/MD/TXT embeddings
- ‚úÖ Semantic document search
- ‚úÖ File-based indexing
- **Timeline:** 5-6 t√Ωdn≈Ø

### **v1.3.0 - OCR & Screenshot Search** üî• HIGHLY RECOMMENDED
- PaddleOCR integration (3 MB)
- Screenshot text extraction
- Business card organization
- Combined text + image search
- **Timeline:** 2-3 t√Ωdny

### **v1.3.x - Quick Wins**
- Metadata embeddings (reuse MiniLM)
- UI element search improvements (reuse CLIP)
- Query suggestions
- **Timeline:** 1-2 t√Ωdny

### **v1.4.0 - Audio & Voice**
- Wav2Vec2 integration (50-100 MB)
- Voice memo search
- Audio similarity
- Speaker recognition
- **Timeline:** 3-4 t√Ωdny

### **v1.5.0 - Enhanced Video**
- Video transcript embeddings (Whisper)
- Combined frame + transcript search
- Multi-modal video search
- **Timeline:** 2-3 t√Ωdny

### **v2.0.0 - Performance & Research**
- MobileCLIP upgrade (research license first)
- 2-5x faster inference
- Reduced battery consumption
- Advanced features (time series, patterns)
- **Timeline:** TBD

---

## üí° QUICK WINS - Nejjednodu≈°≈°√≠ implementace

### 1. **UI Element Search** (3-5 dn≈Ø)
- Zero new models
- Reuse CLIP
- Pouze UX improvements

### 2. **Metadata Search** (1 t√Ωden)
- Reuse MiniLM-L6-v2
- EXIF extraction (u≈æ m√°≈°)
- Minimal code changes

### 3. **Query Suggestions** (2-3 dny)
- Hardcoded suggestions
- "Try searching for..."
- Better UX

---

## üöÄ KILLER FEATURES - Nejvy≈°≈°√≠ hodnota

### 1. **OCR Screenshot Search** üî•
- **Proƒç:** Ka≈æd√Ω m√° stovky screenshot≈Ø, nikdo je neum√≠ naj√≠t
- **Konkurence:** T√©mƒõ≈ô ≈æ√°dn√° Android app to nem√°
- **Model:** Pouze 3 MB
- **Value:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

### 2. **Voice Memo Search**
- **Proƒç:** Unique feature
- **Konkurence:** Velmi m√°lo apps
- **Use case:** Profesion√°ln√≠ users (novin√°≈ôi, studenti)
- **Value:** ‚≠ê‚≠ê‚≠ê‚≠ê

### 3. **Document Search** (u≈æ pl√°nuje≈°)
- **Proƒç:** PDF search je standard, ale m√°lo apps m√° semantic search
- **Value:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

## ‚ö†Ô∏è D≈ÆLE≈ΩIT√â POZN√ÅMKY

### Model Size Management
**Celkov√° velikost model≈Ø:**
```
Current (v1.1.3):
- CLIP IMAGE_ENCODER: ~75 MB
- CLIP TEXT_ENCODER: ~75 MB
Total: ~150 MB

After v1.2.0 (Document Search):
- + MiniLM-L6-v2: +23 MB
Total: ~173 MB

After v1.3.0 (OCR):
- + PaddleOCR: +3 MB
Total: ~176 MB

After v1.4.0 (Audio):
- + Wav2Vec2: +50-100 MB
Total: ~226-276 MB

After v1.5.0 (Video Transcript):
- + Whisper-tiny: +40 MB
Total: ~266-316 MB
```

**Recommendations:**
- ‚úÖ Opt-in model downloads (user choice)
- ‚úÖ Model pruning (remove unused models)
- ‚úÖ Quantization (FP16, INT8)
- ‚úÖ On-demand loading (lazy initialization)

### Memory Management
**Concurrent models:**
```kotlin
// Bad: Load all models at once
val clipModel = loadCLIP()
val textModel = loadMiniLM()
val ocrModel = loadPaddleOCR()
val audioModel = loadWav2Vec2()
// Total RAM: ~500 MB+

// Good: Lazy loading + unloading
val modelManager = ModelManager()
modelManager.loadOnDemand(ModelType.OCR)
// Use OCR model
modelManager.unload(ModelType.OCR)
```

**Best practices:**
- Only load models when needed
- Unload models after use
- Monitor memory pressure
- Implement model cache

### Privacy Considerations
**Sensitive data:**
- ‚ö†Ô∏è OCR m≈Ø≈æe extrahovat personal info (passwords, credit cards)
- ‚ö†Ô∏è Audio embeddings = voice recording privacy
- ‚ö†Ô∏è Metadata m≈Ø≈æe obsahovat location data
- ‚ö†Ô∏è Time series = behavioral tracking

**Privacy-preserving design:**
- ‚úÖ On-device processing (≈æ√°dn√Ω cloud)
- ‚úÖ Optional features (user consent)
- ‚úÖ Data encryption (embeddings at rest)
- ‚úÖ Clear privacy policy

### F-Droid Compatibility Checklist
**Pro ka≈æd√Ω nov√Ω model:**
- [ ] Open-source license (Apache, MIT, BSD)
- [ ] No proprietary dependencies
- [ ] Reproducible builds
- [ ] No telemetry/tracking
- [ ] Source code available

**Problematic models:**
- ‚ùå Google ML Kit (proprietary)
- ‚ö†Ô∏è MobileCLIP (license unclear)
- ‚ùå Closed-source embeddings APIs

---

## üîó HLAVN√ç ZDROJE & REFERENCE

### OCR
- [PaddleOCR GitHub](https://github.com/PaddlePaddle/PaddleOCR)
- [PaddleOCR ONNX Conversion](https://github.com/RapidAI/PaddleOCRModelConvert)
- [PaddleOCR Docs](https://paddlepaddle.github.io/PaddleOCR/)

### Audio Embeddings
- [Wav2Vec2 ONNX Collection](https://huggingface.co/darjusul/wav2vec2-ONNX-collection)
- [Wespeaker (Speaker Embeddings)](https://github.com/wenet-e2e/wespeaker)
- [Whisper GitHub](https://github.com/openai/whisper)
- [Whisper.cpp (lightweight)](https://github.com/ggerganov/whisper.cpp)

### Text Embeddings
- [Sentence-Embeddings-Android](https://github.com/shubham0204/Sentence-Embeddings-Android)
- [all-MiniLM-L6-v2 ONNX](https://huggingface.co/onnx-models/all-MiniLM-L6-v2-onnx)
- [Sentence Transformers](https://www.sbert.net/)

### MobileCLIP
- [MobileCLIP GitHub](https://github.com/apple/ml-mobileclip)
- [Apple ML Research](https://machinelearning.apple.com/research/mobileclip)
- [arXiv Paper](https://arxiv.org/abs/2311.17049)

### Time Series
- [Tiny Time Mixers (IBM)](https://huggingface.co/ibm/TTM)
- [TimesFM (Google)](https://github.com/google-research/timesfm)
- [Chronos (Amazon)](https://github.com/amazon-science/chronos-forecasting)

### ONNX Runtime
- [ONNX Runtime Mobile](https://onnxruntime.ai/docs/tutorials/mobile/)
- [ONNX Model Zoo](https://github.com/onnx/models)
- [HuggingFace Optimum](https://huggingface.co/docs/optimum/onnxruntime/modeling_ort)

### Research Papers (2024-2025)
- [MobileCLIP Paper](https://arxiv.org/abs/2311.17049)
- [MobileViCLIP (Video)](https://arxiv.org/html/2508.07312v1)
- [GNN for Android App Prediction](https://www.nature.com/articles/s41598-025-05260-1)
- [Audio Processing Research](https://www.sciencedirect.com/science/article/abs/pii/S0167639324000761)

---

## üéØ DOPORUƒåEN√ç PRO JAROSLAVA

### Immediate Actions (po v1.2.0 Document Search)

**1. Implementuj OCR embeddings (v1.3.0)** üî•
- Nejlep≈°√≠ value/effort ratio
- Killer feature (screenshot search)
- Mal√Ω model (3 MB)
- 2-3 t√Ωdny pr√°ce

**2. Quick wins (v1.3.x)**
- Metadata search (1 t√Ωden)
- UI element search improvements (3-5 dn≈Ø)
- Query suggestions (2-3 dny)

**3. Research MobileCLIP license**
- Ovƒõ≈ô F-Droid compatibility
- Pokud OK ‚Üí upgrade CLIP v v2.0.0
- 2-5x performance boost

### Medium-term (v1.4.0+)

**4. Audio embeddings (pokud m√° smysl pro user base)**
- Voice memo search
- 3-4 t√Ωdny pr√°ce
- Unique feature

### Long-term Research

**5. Time series behavioral insights**
- Zaj√≠mav√© pro future
- Privacy-preserving design kritick√Ω
- Mo≈æn√° collaboration s research institutions

---

## ‚úÖ Z√ÅVƒöR

**SmartScan m√° potenci√°l st√°t se THE Swiss Army Knife pro media organization:**

### Aktu√°ln√≠ stav (v1.1.3):
- ‚úÖ Image search (CLIP)
- ‚úÖ Video frame search (CLIP)
- ‚úÖ Auto-organization (prototypes)

### Po v1.2.0 (Document Search):
- ‚úÖ PDF/MD/TXT search
- ‚úÖ Semantic document organization

### Po v1.3.0 (OCR - HIGHLY RECOMMENDED):
- ‚úÖ Screenshot search üî•
- ‚úÖ Business card organization üî•
- ‚úÖ Handwritten notes
- ‚úÖ Metadata search

### Po v1.4.0 (Audio):
- ‚úÖ Voice memo search
- ‚úÖ Podcast organization
- ‚úÖ Speaker recognition

### Vision (v2.0.0+):
- ‚úÖ Unified multi-modal search
- ‚úÖ Behavioral insights (time series)
- ‚úÖ Performance optimizations (MobileCLIP)
- ‚úÖ Advanced features (contextual recommendations)

**Celkov√© hodnocen√≠:** üü¢ **SmartScan m√° jasnou roadmap pro 2-3 roky development**

---

**Datum vytvo≈ôen√≠:** 2025-10-27
**Autor anal√Ωzy:** Claude Code + Badatel (research specialist)
**Pro:** Jaroslav (SmartScan developer)
